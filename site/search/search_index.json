{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":""},{"location":"#overview","title":"Overview","text":"<p>Resk-Caching is a Bun/TypeScript cache server and library that reduces LLM costs by serving cached responses using vector similarity and safe, observable APIs.</p> <ul> <li>Multiple backends: in-memory, SQLite, Redis</li> <li>Security: JWT, rate limit, optional AES-GCM encryption-at-rest</li> <li>Observability: Prometheus metrics, OpenTelemetry tracing</li> <li>API-first: OpenAPI 3.1, WebSockets for realtime updates</li> </ul> <p>Use the navigation to learn how to get started, configure backends, and integrate with your stack.</p>"},{"location":"SECURITY_MODEL/","title":"Security Model","text":""},{"location":"SECURITY_MODEL/#security-model","title":"Security Model","text":""},{"location":"SECURITY_MODEL/#principles","title":"Principles","text":"<ul> <li>Secrets are server-only (environment variables or a secret manager). No secrets in the frontend.</li> <li>Enforce TLS for all traffic. JWTs are short-lived; rate-limit per user and per IP.</li> <li>Optional cache-at-rest encryption using AES-GCM; recommended when persisting sensitive data.</li> <li>Structured logs with a correlation-id, plus metrics and traces for debugging and forensics.</li> </ul>"},{"location":"SECURITY_MODEL/#authentication-and-authorization","title":"Authentication and authorization","text":"<ul> <li>All <code>/api/*</code> endpoints require a valid JWT. Validate algorithm, expiration (<code>exp</code>), not-before (<code>nbf</code>), and issued-at (<code>iat</code>).</li> <li>Optionally validate <code>aud</code>/<code>iss</code> and implement simple scopes if needed.</li> </ul>"},{"location":"SECURITY_MODEL/#input-hardening","title":"Input hardening","text":"<ul> <li>Strict runtime validation via Zod schemas. Enforce maximum sizes and sanitize inputs.</li> </ul>"},{"location":"SECURITY_MODEL/#rate-limiting","title":"Rate limiting","text":"<ul> <li>Sliding window (default 15 minutes) with per-identity and per-IP ceilings to mitigate abuse.</li> </ul>"},{"location":"SECURITY_MODEL/#data-protection","title":"Data protection","text":"<ul> <li>AES-GCM encryption for cached values when <code>CACHE_ENCRYPTION_KEY</code> is set (32 bytes, base64).</li> <li>Keys derived using strong KDFs; prefer server-side HMAC for cache key generation when needed.</li> </ul>"},{"location":"SECURITY_MODEL/#observability","title":"Observability","text":"<ul> <li>Prometheus metrics exposed at <code>/api/metrics</code> and OpenTelemetry tracing via OTLP.</li> <li>Use dashboards and alerts (latency, error rate, cache hit rate) to maintain SLOs.</li> </ul>"},{"location":"USAGE/","title":"Getting Started","text":""},{"location":"USAGE/#usage","title":"Usage","text":""},{"location":"USAGE/#run-as-a-server-recommended-to-keep-secrets-server-side","title":"Run as a server (recommended to keep secrets server-side)","text":"<ol> <li>Set environment variables (<code>JWT_SECRET</code>, <code>CACHE_BACKEND</code>, etc.)</li> <li>Start: <code>npm run build &amp;&amp; bun run dev</code></li> <li>Call the API with a short-lived JWT</li> </ol> <p>Example requests:</p> <pre><code>curl -H \"Authorization: Bearer test\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"query\":\"Hello\",\"response\":{\"answer\":\"Hi\"},\"ttl\":3600}' \\\n     http://localhost:3000/api/cache\n\ncurl -H \"Authorization: Bearer test\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"query\":\"Hello\"}' \\\n     http://localhost:3000/api/cache/query\n</code></pre>"},{"location":"USAGE/#use-as-a-library","title":"Use as a library","text":"<pre><code>import { selectCache } from \"resk-caching\";\n\nconst cache = selectCache();\nawait cache.set(\"key\", { payload: true }, 60);\nconst val = await cache.get(\"key\");\n</code></pre>"},{"location":"USAGE/#openapi-and-clients","title":"OpenAPI and clients","text":"<ul> <li>Fetch the spec: <code>GET /api/openapi.json</code></li> <li>Use any OpenAPI generator to produce clients/SDKs.</li> </ul>"},{"location":"USAGE/#environment-variables","title":"Environment variables","text":"<ul> <li><code>PORT</code> (default 3000)</li> <li><code>JWT_SECRET</code></li> <li><code>CACHE_BACKEND</code> = <code>memory</code> | <code>sqlite</code> | <code>redis</code></li> <li><code>REDIS_URL</code> (for Redis backend)</li> <li><code>CACHE_ENCRYPTION_KEY</code> (base64, 32 bytes)</li> <li><code>RATE_LIMIT_WINDOW_MS</code> (default 900000), <code>RATE_LIMIT_MAX</code> (default 1000)</li> </ul>"},{"location":"api/","title":"API","text":""},{"location":"api/#api","title":"API","text":"<p>All authenticated endpoints expect <code>Authorization: Bearer &lt;token&gt;</code>.</p>"},{"location":"api/#health","title":"Health","text":"<pre><code>GET /health\n</code></pre>"},{"location":"api/#core-cache","title":"Core cache","text":"<pre><code>POST /api/cache        # store key-value\nPOST /api/cache/query  # retrieve by key\nDELETE /api/cache      # clear all\n</code></pre>"},{"location":"api/#semantic","title":"Semantic","text":"<pre><code>POST /api/semantic/store    # store responses with embeddings\nPOST /api/semantic/search   # search by query+embedding\nGET  /api/semantic/responses?query=...  # list stored responses for a query\nGET  /api/semantic/stats     # stats and performance\n</code></pre>"},{"location":"api/#observability","title":"Observability","text":"<pre><code>GET /api/openapi.json\nGET /api/metrics\n</code></pre> <p>See <code>docs/USAGE.md</code> for end-to-end examples and curl snippets.</p>"},{"location":"backends/","title":"Backends","text":""},{"location":"backends/#cache-backends","title":"Cache backends","text":"<p>Set <code>CACHE_BACKEND=memory | sqlite | redis</code>.</p>"},{"location":"backends/#in-memory-memory","title":"In-memory (<code>memory</code>)","text":"<ul> <li>Fastest single-process Map-based storage.</li> <li>TTL kept per-entry; expired entries removed lazily during <code>get</code>.</li> <li>No cross-process sharing, no durability.</li> </ul> <p>Code reference:</p> <pre><code>// src/cache/in-memory.ts\nclass InMemoryCache { /* Map&lt;string, { value, expiresAt }&gt; */ }\n</code></pre>"},{"location":"backends/#sqlite-sqlite","title":"SQLite (<code>sqlite</code>)","text":"<ul> <li>Durable local store using Bun's SQLite.</li> <li>Table: <code>kv(key TEXT PRIMARY KEY, value TEXT, expiresAt INTEGER)</code>.</li> <li><code>set</code>: UPSERTs JSON-serialized value; <code>expiresAt</code> computed from TTL.</li> <li><code>get</code>: lazily deletes expired rows, returns parsed JSON.</li> <li>Default path: <code>resk-cache.sqlite</code>.</li> </ul> <p>Code reference:</p> <pre><code>// src/cache/sqlite.ts\nexport class SqliteCache { /* run/query over kv table */ }\n</code></pre>"},{"location":"backends/#redis-redis","title":"Redis (<code>redis</code>)","text":"<ul> <li>Distributed cache via Bun's RESP3 client.</li> <li>JSON-serialized values; optional TTL via <code>EXPIRE</code>.</li> <li>Key prefix: <code>rc:</code>. <code>clear()</code> scans and deletes only <code>rc:*</code> keys.</li> <li>Helpers: round-robin counters, sets/lists for variants, basic pub/sub.</li> </ul> <p>Code reference:</p> <pre><code>// src/cache/redis-bun.ts\nexport class RedisBunCache { /* get/set/clear + helpers */ }\n</code></pre>"},{"location":"backends/#embeddings-management-important","title":"Embeddings management (important)","text":"<ul> <li>Embeddings for semantic search are NOT stored in SQLite/Redis by default.</li> <li>Semantic features use an in-memory vector store:</li> </ul> <pre><code>// src/cache/vector-memory.ts\nexport class InMemoryVectorCache implements VectorSearchBackend {\n  private entries = new Map&lt;string, CachedLLMEntry&gt;();\n  private embeddings = new Map&lt;string, number[]&gt;();\n  // storeLLMResponse(): keeps entry + query_embedding.vector in memory\n  // searchSimilar(): cosine similarity across stored embeddings\n}\n</code></pre> <ul> <li>If you need persistence or large-scale vector search, use an external vector database (e.g., Pinecone, Weaviate, Qdrant). See <code>docs/ingestion.md</code> for ingestion and schema guidance.</li> <li>The core key-value cache backends (memory/sqlite/redis) are used for generic caching via <code>/api/cache</code> and <code>selectCache()</code>. The semantic routes (<code>/api/semantic/*</code>) currently use the in-memory vector cache only.</li> </ul>"},{"location":"backends/#security-wrapper","title":"Security wrapper","text":"<ul> <li>All key-value backends selected via <code>selectCache()</code> are wrapped by <code>SecureCacheWrapper</code> which encrypts values at rest when <code>CACHE_ENCRYPTION_KEY</code> is set.</li> <li>The in-memory vector cache stores objects in clear by design; do not place secrets in semantic entries.</li> </ul> <p>Code reference:</p> <pre><code>// src/cache/index.ts\nexport function selectCache(): CacheBackend { /* wraps chosen backend with SecureCacheWrapper */ }\n\n// src/cache/secure-wrapper.ts\nexport class SecureCacheWrapper { /* encryptForCache/decryptFromCache */ }\n</code></pre>"},{"location":"backends/#environment","title":"Environment","text":"<pre><code>export CACHE_BACKEND=redis   # memory | sqlite | redis\nexport REDIS_URL=redis://localhost:6379\nexport CACHE_ENCRYPTION_KEY=base64_32_bytes_key\n</code></pre> <p>See also: <code>docs/ingestion.md</code> for vector DB setup and ingestion pipeline.</p>"},{"location":"comparison/","title":"Comparison","text":""},{"location":"comparison/#how-were-different","title":"How we're different","text":"<ul> <li>GPTCache: Python-first. Resk-Caching targets Bun/TypeScript with a secured HTTP API, OpenAPI, metrics, tracing, and optional encryption.</li> <li>ModelCache: Adds semantic caching. Resk-Caching includes production-grade concerns (rate limit, JWT, OTEL/Prometheus, WebSockets) and pluggable backends.</li> <li>Upstash Semantic Cache: Managed service. Resk-Caching is open-source, self-hosted by default, supports local dev with SQLite/memory.</li> <li>Redis LangCache: Managed Redis-based cache. Resk-Caching supports Redis natively and also SQLite/memory for portability.</li> <li>SemantiCache (FAISS): FAISS-native. Resk-Caching prioritizes a secure, observable API surface and external vector DB adapters without GPU requirements.</li> </ul>"},{"location":"deployment/","title":"Deployment","text":""},{"location":"deployment/#deployment","title":"Deployment","text":""},{"location":"deployment/#docker","title":"Docker","text":"<p>Build and run:</p> <pre><code>docker build -t resk-caching:latest .\n\ndocker run --rm -p 3000:3000 \\\n  -e JWT_SECRET=change_me \\\n  -e CACHE_BACKEND=redis \\\n  -e REDIS_URL=redis://redis:6379 \\\n  -e CACHE_ENCRYPTION_KEY=base64_32_bytes_key \\\n  --name resk-caching resk-caching:latest\n</code></pre> <p>Kubernetes (snippet):</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: resk-caching\nspec:\n  replicas: 2\n  selector:\n    matchLabels: { app: resk-caching }\n  template:\n    metadata:\n      labels: { app: resk-caching }\n    spec:\n      containers:\n        - name: server\n          image: ghcr.io/resk-security/resk-caching:latest\n          ports: [{ containerPort: 3000 }]\n          env:\n            - { name: JWT_SECRET, valueFrom: { secretKeyRef: { name: resk-secrets, key: jwt } } }\n            - { name: CACHE_BACKEND, value: redis }\n            - { name: REDIS_URL, value: redis://redis:6379 }\n            - { name: CACHE_ENCRYPTION_KEY, valueFrom: { secretKeyRef: { name: resk-secrets, key: cacheKey } } }\n          readinessProbe:\n            httpGet: { path: /health, port: 3000 }\n            initialDelaySeconds: 5\n            periodSeconds: 10\n          livenessProbe:\n            httpGet: { path: /health, port: 3000 }\n            initialDelaySeconds: 10\n            periodSeconds: 20\n</code></pre>"},{"location":"deployment/#configuration","title":"Configuration","text":"<ul> <li><code>PORT</code> (default 3000)</li> <li><code>JWT_SECRET</code> (required for protected endpoints)</li> <li><code>CACHE_BACKEND</code> = <code>memory|sqlite|redis</code></li> <li><code>REDIS_URL</code> when using Redis</li> <li><code>CACHE_ENCRYPTION_KEY</code> base64 32 bytes to encrypt at rest (key-value cache backends)</li> <li><code>RATE_LIMIT_WINDOW_MS</code>, <code>RATE_LIMIT_MAX</code></li> <li><code>OTEL_EXPORTER_OTLP_ENDPOINT</code>, <code>OTEL_SERVICE_NAME</code></li> </ul>"},{"location":"deployment/#notes","title":"Notes","text":"<ul> <li>For SQLite, mount a volume to persist <code>resk-cache.sqlite</code>.</li> <li>For Redis, prefer a managed service or a StatefulSet.</li> <li>Distroless runtime is used; logs go to stdout.</li> </ul>"},{"location":"ingestion/","title":"Ingestion","text":""},{"location":"ingestion/#ingestion-and-vector-database","title":"Ingestion and vector database","text":"<p>Semantic features require embeddings. By default, semantic storage/search runs in-memory (<code>InMemoryVectorCache</code>). For persistence and scale, ingest into an external vector DB (e.g., Pinecone, Weaviate, Qdrant, Chroma).</p>"},{"location":"ingestion/#workflow","title":"Workflow","text":"<ol> <li>Collect documents or canonical Q&amp;A responses.</li> <li>Chunk long texts (by sentence, tokens, or structure) to 100-500 tokens.</li> <li>Generate embeddings for each chunk (OpenAI, HF, etc.).</li> <li>Upsert vectors with metadata into your vector DB.</li> <li>At query-time, embed the query and run a similarity search; map the result back to your stored LLM responses.</li> </ol>"},{"location":"ingestion/#embeddings","title":"Embeddings","text":"<ul> <li>Provider: <code>EMBEDDING_PROVIDER=openai|huggingface</code></li> <li>Model: <code>EMBEDDING_MODEL</code></li> </ul>"},{"location":"ingestion/#example-script","title":"Example script","text":"<p>Use the provided example:</p> <pre><code>bun run scripts/ingest-example.ts\n</code></pre> <p>Key code:</p> <pre><code>// scripts/ingest-example.ts\n// - chunkText(): naive chunking\n// - embedBatch(): OpenAI/HF embeddings\n// - upsertPinecone(): Pinecone REST upsert\n</code></pre>"},{"location":"ingestion/#pinecone-schema","title":"Pinecone schema","text":"<ul> <li>Each vector: <code>{ id: string, values: number[], metadata?: Record&lt;string, any&gt; }</code></li> <li>Recommended metadata: <code>{ source, parentId, title, category, tags, timestamp }</code></li> <li>Namespace per domain or customer if needed.</li> </ul>"},{"location":"ingestion/#weaviateqdrantchroma","title":"Weaviate/Qdrant/Chroma","text":"<ul> <li>Weaviate: store as <code>class</code> objects with vector and properties.</li> <li>Qdrant: <code>points</code> with <code>vector</code> and <code>payload</code>.</li> <li>Chroma: <code>collection.add(documents, metadatas, ids, embeddings)</code>.</li> </ul>"},{"location":"ingestion/#query-time","title":"Query-time","text":"<ol> <li>Compute query embedding.</li> <li>Search top-k in your vector DB (threshold and filters as needed).</li> <li>Retrieve stored responses for the matched entry/query.</li> <li>Apply variant selection (<code>random|round-robin|deterministic|weighted</code>).</li> </ol>"},{"location":"ingestion/#mapping-between-vector-db-and-responses","title":"Mapping between vector DB and responses","text":"<ul> <li>You can store your <code>CachedLLMEntry</code> in your own DB (e.g., Postgres) and link it via the vector metadata <code>entryId</code> or <code>query</code>.</li> <li>Alternatively, call <code>/api/semantic/store</code> to populate the in-memory vector cache during boot, then rely on your external vector DB for search.</li> </ul>"},{"location":"ingestion/#example-request-store-entries","title":"Example request: store entries","text":"<pre><code>curl -X POST http://localhost:3000/api/semantic/store \\\n  -H \"Authorization: Bearer $JWT\" -H \"Content-Type: application/json\" \\\n  -d '{\n    \"query\": \"thank you\",\n    \"query_embedding\": { \"vector\": [0.1,0.2,0.3], \"dimension\": 3 },\n    \"responses\": [ { \"id\": \"resp1\", \"text\": \"You're welcome!\" } ],\n    \"variant_strategy\": \"weighted\",\n    \"weights\": [1]\n  }'\n</code></pre>"},{"location":"ingestion/#example-request-search","title":"Example request: search","text":"<pre><code>curl -X POST http://localhost:3000/api/semantic/search \\\n  -H \"Authorization: Bearer $JWT\" -H \"Content-Type: application/json\" \\\n  -d '{\n    \"query\": \"merci\",\n    \"query_embedding\": { \"vector\": [0.11,0.19,0.29], \"dimension\": 3 },\n    \"limit\": 3,\n    \"similarity_threshold\": 0.7\n  }'\n</code></pre>"},{"location":"ingestion/#notes","title":"Notes","text":"<ul> <li>The in-memory vector store is process-local; it resets on restart.</li> <li>For production-grade semantic search, prefer an external vector DB.</li> <li>Keep embedding dimension consistent across your pipeline.</li> </ul>"},{"location":"next-steps/","title":"Next Steps","text":""},{"location":"next-steps/#next-steps","title":"Next steps","text":"<ul> <li>Docker image and multi-stage build</li> <li>LangChain integration helper and examples</li> <li>LlamaIndex and Vercel AI SDK adapters</li> <li>Pluggable vector stores (Qdrant, Weaviate, Pinecone)</li> <li>Stale-while-revalidate and background refresh</li> <li>Eviction strategies (LRU/LFU) and warming CLI</li> <li>Upstash Redis &amp; Redis Cloud deployment templates</li> <li>Benchmarks and load tests (k6/Artillery)</li> </ul>"}]}